# API Blackbox - Gateway OpenRouter com Governan√ßa

Gateway inteligente para modelos LLM via OpenRouter, com controle de custos, mascaramento de PII e observabilidade completa.

---

## üéØ Conceitos e Por Qu√™

### O que √© um API Gateway?

Um **API Gateway** √© uma camada intermedi√°ria entre sua aplica√ß√£o e servi√ßos externos (neste caso, modelos LLM). Ele centraliza:
- **Autentica√ß√£o e autoriza√ß√£o**
- **Rate limiting e controle de custos**
- **Logs e observabilidade**
- **Transforma√ß√£o de dados**
- **Governan√ßa e compliance**

### Por que "Blackbox"?

O termo "blackbox" refere-se √† **opacidade intencional** dos dados sens√≠veis:
- Dados PII s√£o **mascarados** antes de enviar ao LLM
- O LLM nunca v√™ informa√ß√µes sens√≠veis reais
- Logs armazenam vers√µes mascaradas (compliance com LGPD/GDPR)

### Por que OpenRouter?

**OpenRouter** √© um gateway que agrega m√∫ltiplos provedores LLM:
- **Uma API, dezenas de modelos**: GPT-4, Claude, Llama, Gemini, etc
- **Billing centralizado**: Uma fatura, m√∫ltiplos providers
- **Sem vendor lock-in**: Mude de modelo sem reescrever c√≥digo
- **Fallback autom√°tico**: Se um modelo falha, tenta outro

### Problemas Resolvidos

**1. Custos Descontrolados üí∏**
- LLMs cobram por token (~$0.50 a $30 por 1M tokens)
- Loops infinitos ou bugs podem custar milhares
- **Solu√ß√£o**: Rate limit di√°rio (ex: US$ 15/dia)

**2. Dados Sens√≠veis üîí**
- CPFs, CNPJs, emails podem vazar em logs
- LGPD/GDPR exigem prote√ß√£o de PII
- **Solu√ß√£o**: Mascaramento autom√°tico via regex

**3. Falta de Observabilidade üìä**
- Dif√≠cil rastrear quem est√° usando quanto
- Logs dispersos e n√£o estruturados
- **Solu√ß√£o**: Logs centralizados no PostgreSQL com tracking de custos

**4. M√∫ltiplos Providers üîÄ**
- APIs diferentes para OpenAI, Anthropic, Meta, Google
- Gerenciamento de m√∫ltiplas chaves complexo
- **Solu√ß√£o**: OpenRouter unifica todos em uma API

---

## üèóÔ∏è Arquitetura do Sistema

### Fluxo de Requisi√ß√£o

```
Cliente ‚Üí FastAPI ‚Üí Middleware (PII Mask + Cost Limit) ‚Üí OpenRouter ‚Üí LLM
                         ‚Üì
                  PostgreSQL (logs + costs)
```

### Componentes Principais

#### 1. API (FastAPI)

**Endpoints**:
- `POST /chat` - Chat completion
- `GET /models` - Lista modelos dispon√≠veis
- `GET /health` - Health check

#### 2. Middleware

**PII Masker**:
- Detecta CPF, CNPJ, email, telefone via regex
- Substitui por m√°scaras antes de enviar ao LLM
- LLM nunca v√™ dados sens√≠veis

**Cost Limiter**:
- Calcula custo estimado por request
- Soma com gasto do dia atual
- Bloqueia se >= limite (retorna 429)

#### 3. Guardrails (opcional)

- **Injection Detector**: Detecta prompt injection
- **Topic Validator**: Valida se t√≥pico √© permitido
- **Output Validator**: Valida formato da resposta

#### 4. Observabilidade

**Tabela `observability.llm_logs`**:
- Prompt mascarado
- Resposta mascarada
- Tokens usados
- Custo em USD
- Lat√™ncia em ms
- Timestamp

**Tabela `observability.spend_ledger`**:
- Hash da API key (seguran√ßa)
- Modelo usado
- Custo em USD
- Timestamp

### Stack Tecnol√≥gica

- **FastAPI**: Framework web moderno e ass√≠ncrono
- **OpenRouter**: Gateway multi-modelo
- **PostgreSQL**: Logs e tracking de custos
- **Pydantic**: Valida√ß√£o de dados
- **Docker**: Containeriza√ß√£o

---

## üöÄ Deploy Local

### Pr√©-requisitos

- Docker e Docker Compose
- PostgreSQL (via Docker ou instalado)
- Python 3.11+ (opcional, para desenvolvimento)
- OpenRouter API Key ([obtenha aqui](https://openrouter.ai))

### Op√ß√£o 1: Docker Compose (Recomendado para Produ√ß√£o Local)

```bash
# 1. Configure vari√°veis de ambiente
cd pipelines/api-blackbox
cp .env.example .env
# Edite .env com OPENROUTER_API_KEY e DATABASE_URL

# 2. Suba o servi√ßo
docker-compose up -d

# 3. Verifique logs
docker-compose logs -f api-blackbox

# 4. Teste
curl http://localhost:8000/health
```

**docker-compose.yml** (criar na raiz de `pipelines/api-blackbox/`):
```yaml
version: '3.8'

services:
  api-blackbox:
    build:
      context: ../..
      dockerfile: pipelines/api-blackbox/Dockerfile.prod
    ports:
      - "8000:8080"
    environment:
      - DATABASE_URL=${DATABASE_URL}
      - OPENROUTER_API_KEY=${OPENROUTER_API_KEY}
      - BUDGET_USD_DAY=${BUDGET_USD_DAY:-15.0}
      - APP_ENV=production
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Opcional: PostgreSQL local
  postgres:
    image: pgvector/pgvector:pg15
    ports:
      - "5432:5432"
    environment:
      - POSTGRES_PASSWORD=postgres
      - POSTGRES_DB=llmops
    volumes:
      - postgres_data:/var/lib/postgresql/data

volumes:
  postgres_data:
```

### Op√ß√£o 2: Python Local (Desenvolvimento)

```bash
# 1. Configure ambiente
cd pipelines/api-blackbox
python -m venv .venv
source .venv/bin/activate  # Linux/Mac
# ou .venv\Scripts\activate  # Windows

# 2. Instale depend√™ncias
pip install -r requirements.txt

# 3. Configure vari√°veis de ambiente
export OPENROUTER_API_KEY=sk-or-v1-...
export DATABASE_URL=postgresql+asyncpg://user:pass@localhost:5432/llmops
export BUDGET_USD_DAY=15.0

# 4. Execute a API
cd app
python main.py
```

### Op√ß√£o 3: Deploy em VPS/VM

Para deploy em servidor pr√≥prio:

```bash
# 1. Instale Docker no servidor
curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh

# 2. Clone o reposit√≥rio
git clone https://github.com/seu-repo/learning-llmops.git
cd learning-llmops/pipelines/api-blackbox

# 3. Configure .env
nano .env

# 4. Suba com Docker Compose
docker-compose up -d

# 5. Configure Nginx como reverse proxy
sudo apt install nginx
sudo nano /etc/nginx/sites-available/api-blackbox
```

**Nginx config**:
```nginx
server {
    listen 80;
    server_name api.seudominio.com;

    location / {
        proxy_pass http://localhost:8000;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
    }

    # Rate limiting (adicional ao rate limit da API)
    limit_req_zone $binary_remote_addr zone=api:10m rate=10r/s;
    limit_req zone=api burst=20 nodelay;
}
```

```bash
# Ativar site
sudo ln -s /etc/nginx/sites-available/api-blackbox /etc/nginx/sites-enabled/
sudo nginx -t
sudo systemctl reload nginx
```

---

## ‚òÅÔ∏è Deploy Cloud (GCP)

Para deploy em produ√ß√£o no Google Cloud Platform, veja [DEPLOY.md](./DEPLOY.md) para:
- Build autom√°tico com Cloud Build
- Deploy no Cloud Run
- Versionamento no Artifact Registry
- CI/CD com triggers
- Rollback e recupera√ß√£o

**Quick deploy**:
```bash
cd pipelines/api-blackbox
bash deploy.sh
```

---

## üì° Uso da API

### Chat Completion

```bash
POST /chat
Content-Type: application/json

{
  "messages": [
    {"role": "user", "content": "Qual a cota√ß√£o da PETR4?"}
  ],
  "model": "gpt-oss-120b",
  "temperature": 0.7,
  "max_tokens": 500
}
```

**Exemplo**:
```bash
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "Explique LLMOps em 2 frases"}
    ],
    "model": "gpt-oss-120b"
  }'
```

**Resposta**:
```json
{
  "message": {
    "role": "assistant",
    "content": "LLMOps √© a pr√°tica de operacionalizar modelos LLM..."
  },
  "model": "gpt-oss-120b",
  "usage": {
    "prompt_tokens": 15,
    "completion_tokens": 30,
    "total_tokens": 45
  },
  "cost_usd": 0.0075,
  "latency_ms": 1234,
  "created_at": "2024-11-14T15:30:00Z",
  "request_id": "uuid-here"
}
```

### Modelos Dispon√≠veis

```bash
GET /models
```

```bash
curl http://localhost:8000/models
```

**Resposta**:
```json
{
  "models": [
    {
      "id": "gpt-oss-120b",
      "name": "GPT OSS 120B",
      "provider": "Together",
      "input_cost_per_1k": 0.10,
      "output_cost_per_1k": 0.20,
      "context_window": 4096,
      "description": "Modelo open-source econ√¥mico"
    },
    {
      "id": "anthropic/claude-3.5-sonnet",
      "name": "Claude 3.5 Sonnet",
      "provider": "Anthropic",
      "input_cost_per_1k": 3.00,
      "output_cost_per_1k": 15.00,
      "context_window": 200000,
      "description": "Modelo premium de alta qualidade"
    }
  ]
}
```

### Health Check

```bash
GET /health
```

```bash
curl http://localhost:8000/health
```

---

## üîí Funcionalidades de Governan√ßa

### 1. Mascaramento de PII

**O que √© mascarado**:
- CPF: `123.456.789-00` ‚Üí `***.***.**-**`
- CNPJ: `12.345.678/0001-00` ‚Üí `**.***.***/****-**`
- Email: `user@example.com` ‚Üí `***@***.***`
- Telefone: `(11) 98765-4321` ‚Üí `(11) *****-****`

**Exemplo**:
```python
# Input do usu√°rio
"Meu CPF √© 123.456.789-00 e email joao@gmail.com"

# Enviado ao LLM (mascarado)
"Meu CPF √© ***.***.**-** e email ***@***.***"

# Log no banco (mascarado)
prompt_masked: "Meu CPF √© ***.***.**-** e email ***@***.***"
```

### 2. Controle de Custos

**Limite di√°rio configur√°vel**:
```env
BUDGET_USD_DAY=15.0
```

**Fluxo**:
1. Request chega
2. Calcula custo estimado
3. Soma com gasto do dia (tabela `spend_ledger`)
4. Se >= limite ‚Üí retorna `429 Too Many Requests`
5. Se < limite ‚Üí processa normalmente
6. Ap√≥s processamento ‚Üí registra custo real

**C√°lculo de custo**:
```python
custo_input = (prompt_tokens / 1000) √ó pre√ßo_input_por_1k
custo_output = (completion_tokens / 1000) √ó pre√ßo_output_por_1k
custo_total = custo_input + custo_output
```

### 3. Observabilidade

**Logs estruturados**:
Cada requisi√ß√£o salva em `observability.llm_logs`:
- Prompt mascarado
- Resposta mascarada
- Tokens usados
- Custo em USD
- Lat√™ncia em ms
- Status (success/error)

**Tracking de custos**:
Cada requisi√ß√£o salva em `observability.spend_ledger`:
- Hash da API key (SHA256)
- Modelo usado
- Custo em USD
- Timestamp

**Consultas √∫teis**:
```sql
-- Custo por dia
SELECT
  DATE(ts) as dia,
  SUM(cost_usd) as custo_total
FROM observability.spend_ledger
GROUP BY DATE(ts)
ORDER BY dia DESC;

-- Custo por modelo
SELECT
  model,
  COUNT(*) as requests,
  SUM(cost_usd) as custo_total,
  AVG(cost_usd) as custo_medio
FROM observability.spend_ledger
WHERE ts >= CURRENT_DATE - INTERVAL '7 days'
GROUP BY model
ORDER BY custo_total DESC;
```

---

## üìÅ Arquivos de Deploy

### Dockerfile.dev

Imagem Docker para **desenvolvimento local**:
- Hot reload autom√°tico
- Logs verbosos (DEBUG)
- Sem otimiza√ß√µes de produ√ß√£o

```bash
docker build -f Dockerfile.dev -t api-blackbox:dev .
docker run -p 8000:8000 --env-file .env api-blackbox:dev
```

### Dockerfile.prod

Imagem Docker **otimizada para produ√ß√£o**:
- Multi-stage build (menor tamanho)
- Usu√°rio n√£o-root (seguran√ßa)
- Health checks integrados
- Sem ferramentas de desenvolvimento

```bash
docker build -f Dockerfile.prod -t api-blackbox:prod .
docker run -p 8080:8080 --env-file .env api-blackbox:prod
```

### cloudbuild.yaml

Configura√ß√£o de **CI/CD para GCP**:
- Build autom√°tico da imagem
- Push para Artifact Registry (versionamento)
- Deploy no Cloud Run

**Processo**: Build ‚Üí Push ‚Üí Deploy

### deploy.sh

Script de **deploy automatizado**:
- Valida√ß√µes de ambiente e credenciais
- Deploy interativo com confirma√ß√µes
- Exibe informa√ß√µes p√≥s-deploy

```bash
bash deploy.sh
```

---

## üîß Troubleshooting

### Erro: OPENROUTER_API_KEY n√£o encontrada

**Sintoma**: `500 Internal Server Error: Missing OPENROUTER_API_KEY`

**Solu√ß√µes**:
```bash
# .env
OPENROUTER_API_KEY=sk-or-v1-xxx

# Ou Secret Manager (GCP)
gcloud secrets create OPENROUTER_API_KEY --data-file=- <<< "sk-or-v1-xxx"
```

### Erro: 429 - Limite di√°rio excedido

**Sintoma**: `429 Too Many Requests: Daily budget exceeded`

**Solu√ß√µes**:
1. Aumentar limite: `BUDGET_USD_DAY=30.0`
2. Esperar at√© meia-noite UTC (reset autom√°tico)
3. Limpar spend_ledger (apenas dev):
   ```sql
   DELETE FROM observability.spend_ledger WHERE ts >= CURRENT_DATE;
   ```

### Erro: Database n√£o conectado

**Sintoma**: `500 Internal Server Error: Database connection failed`

**Solu√ß√µes**:
```bash
# Verifica conectividade
psql $DATABASE_URL -c "SELECT 1"

# Aplica schemas
make db-init

# Verifica se PostgreSQL est√° rodando
docker ps | grep postgres
```

### Lat√™ncia alta (>5s)

**Causas comuns**:
1. Modelo lento (GPT-4 √© mais lento que GPT-3.5)
2. Prompt muito grande (reduzir contexto)
3. OpenRouter sobrecarregado
4. Rede lenta

**Solu√ß√µes**:
- Use modelos mais r√°pidos: `gpt-oss-120b`, `gpt-3.5-turbo`
- Reduza `max_tokens`
- Aumente timeout: `client.timeout = 120.0`

### PII n√£o est√° sendo mascarado

**Sintoma**: Dados sens√≠veis aparecem nos logs

**Verificar**:
```python
# Testar regex localmente
from app.middleware.pii_masker import mask_pii

text = "Meu CPF √© 123.456.789-00"
masked = mask_pii(text)
print(masked)  # Deve mostrar: "Meu CPF √© ***.***.**-**"
```

**Solu√ß√µes**:
1. Verifique se middleware est√° ativo
2. Verifique formato dos dados (regex pode n√£o detectar formatos incomuns)
3. Adicione novos padr√µes no `pii_masker.py`

---

## üìö Documenta√ß√£o Adicional

- **[DEPLOY.md](./DEPLOY.md)** - Deploy avan√ßado em cloud
- **[PROMPT_LIBRARY.md](./PROMPT_LIBRARY.md)** - Biblioteca de prompts versionados
- **[OpenRouter Docs](https://openrouter.ai/docs)** - Documenta√ß√£o oficial
- **[FastAPI Docs](https://fastapi.tiangolo.com)** - Framework web

---

## ü§ù Contribuindo

Veja [CONTRIBUTING.md](../../CONTRIBUTING.md) na raiz do projeto.

---

**Desenvolvido com ‚ù§Ô∏è pelo Learning LLMOps Team**
